from __future__ import annotations

import numpy as np
import pandas as pd

from typing import Dict, Any, Optional, Tuple, List
from xgboost import XGBClassifier


# ============================================================
# 1) Univariate numeric profiling (pandas)
# ============================================================

def describe_numeric_pandas(
    df: pd.DataFrame,
    exclude: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Compute univariate numeric profiling for a pandas DataFrame.

    For each numeric column (excluding those in `exclude`), this function
    returns a row with:

      - variable: column name
      - n_non_missing: number of non-missing observations
      - missing_pct: percentage of missing values
      - mean: arithmetic mean
      - stddev: sample standard deviation (ddof=1, same as pandas .std())
      - min: minimum value
      - max: maximum value
      - p1: 1st percentile (0.01 quantile)
      - median: 50th percentile (0.50 quantile)
      - p99: 99th percentile (0.99 quantile)

    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame.
    exclude : list of str, optional
        List of column names to exclude from the profiling (IDs, target, etc.).

    Returns
    -------
    pd.DataFrame
        Long-format DataFrame with one row per numeric variable and the
        descriptive statistics listed above.
    """
    exclude = set(exclude or [])
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [c for c in numeric_cols if c not in exclude]

    rows = []

    for col in numeric_cols:
        series = df[col]
        n = len(series)
        n_non_missing = series.count()
        missing_pct = (n - n_non_missing) / n * 100.0

        # Percentiles (will ignore NaNs by default)
        p1, median, p99 = series.quantile([0.01, 0.5, 0.99])

        rows.append(
            {
                "variable": col,
                "n_non_missing": n_non_missing,
                "missing_pct": missing_pct,
                "mean": series.mean(),
                "stddev": series.std(),
                "min": series.min(),
                "max": series.max(),
                "p1": p1,
                "median": median,
                "p99": p99,
            }
        )

    return pd.DataFrame(rows)


# ============================================================
# 2) Univariate Gini / AUC (pandas)
# ============================================================

def _auc_from_series(x: pd.Series, y: pd.Series) -> Optional[float]:
    """
    Compute AUC using the rank-based Mann–Whitney formulation.

    This is equivalent to the Wilcoxon rank-sum statistic-based AUC:

        AUC = (sum of ranks of positive class - n_pos*(n_pos+1)/2) / (n_pos*n_neg)

    Parameters
    ----------
    x : pd.Series
        Predictor values (continuous or discrete).
    y : pd.Series
        Binary target, expected values 0/1 (will not be coerced internally).

    Returns
    -------
    float or None
        Estimated AUC. Returns None if all targets are 0 or all are 1.
    """
    df_tmp = pd.DataFrame({"x": x, "y": y})
    df_tmp = df_tmp.dropna(subset=["x"])

    # Degenerate cases: all negatives or all positives
    if df_tmp["y"].sum() == 0:
        return None
    if df_tmp["y"].sum() == len(df_tmp):
        return None

    # Sort by predictor
    df_tmp = df_tmp.sort_values("x")
    n_pos = int(df_tmp["y"].sum())
    n_neg = len(df_tmp) - n_pos

    # Ranks start at 1
    df_tmp["rank"] = np.arange(1, len(df_tmp) + 1)

    rank_sum_pos = df_tmp.loc[df_tmp["y"] == 1, "rank"].sum()

    auc = (rank_sum_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)
    return float(auc)


def univariate_gini_pandas(
    df: pd.DataFrame,
    target: str,
    exclude: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Compute univariate AUC and Gini for each numeric feature in a pandas DataFrame.

    For each numeric feature (excluding those in `exclude` and the `target`),
    the function computes two Gini coefficients:

      1) gini_no_missing:
         - Drop rows where the feature is missing (NaN).
         - Compute AUC using the rank-based Mann–Whitney approach.
         - Gini = 2 * AUC - 1.

      2) gini_missing_zero:
         - Replace missing feature values with 0.
         - Compute AUC on this imputed version.
         - Gini = 2 * AUC - 1.

    This is particularly useful in credit scoring / risk modeling to understand
    how sensitive the univariate discrimination is to missing-value handling.

    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame.
    target : str
        Name of the binary target column (0/1).
    exclude : list of str, optional
        List of column names to exclude from the analysis (IDs, leakage, etc.).

    Returns
    -------
    pd.DataFrame
        DataFrame with one row per numeric variable and columns:
          - variable
          - auc_no_missing
          - gini_no_missing
          - auc_missing_zero
          - gini_missing_zero
    """
    exclude = set(exclude or [])
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [c for c in numeric_cols if c not in exclude and c != target]

    rows = []

    y = df[target].astype(int)

    for col in numeric_cols:
        x = df[col]

        # 1) Gini dropping missing values
        auc_no = _auc_from_series(x, y)
        gini_no = 2.0 * auc_no - 1.0 if auc_no is not None else None

        # 2) Gini imputing missing values with zero
        x_miss0 = x.fillna(0)
        auc_zero = _auc_from_series(x_miss0, y)
        gini_zero = 2.0 * auc_zero - 1.0 if auc_zero is not None else None

        rows.append(
            {
                "variable": col,
                "auc_no_missing": auc_no,
                "gini_no_missing": gini_no,
                "auc_missing_zero": auc_zero,
                "gini_missing_zero": gini_zero,
            }
        )

    return pd.DataFrame(rows)


# ============================================================
# 3) NAFS-style feature reduction (pandas + XGBoost)
# ============================================================

def nafs_feature_reduction(
    X: pd.DataFrame,
    y: pd.Series,
    n_runs: int = 20,
    noise_multiplier: int = 1,
    noise_quantile: float = 0.95,
    min_selection_freq: float = 0.60,
    random_state: int = 42,
    xgb_kwargs: Optional[Dict[str, Any]] = None,
) -> Tuple[pd.DataFrame, pd.Series, List[str]]:
    """
    NAFS-style feature reduction using XGBoost and shadow/noise features.

    Concept
    -------
    For each run:
      1) Create "shadow" (noise) features by permuting each real feature.
         Optionally repeat this `noise_multiplier` times.
      2) Fit an XGBoost model using real + shadow features.
      3) Compute gain-based feature importance.
      4) Compute a noise threshold as the `noise_quantile` of the shadow
         feature gains.
      5) For each real feature, mark whether its gain exceeds that threshold.

    After `n_runs`:
      - selection_freq[feature] = (# runs where feature beat noise) / n_runs
      - Keep features with selection_freq >= min_selection_freq.
      - If no feature passes the threshold, fallback to keeping all features.

    Parameters
    ----------
    X : pd.DataFrame
        Design matrix with explanatory variables (already encoded, numeric).
    y : pd.Series
        Binary target (0/1).
    n_runs : int
        Number of NAFS model runs.
    noise_multiplier : int
        Number of shadow copies per real feature.
    noise_quantile : float
        Quantile over shadow feature gains used as the noise threshold.
    min_selection_freq : float
        Minimum selection frequency to retain a feature.
    random_state : int
        Base random seed for the noise generator and models.
    xgb_kwargs : dict, optional
        Additional keyword arguments for XGBClassifier. These override
        the internal defaults used for NAFS.

    Returns
    -------
    X_reduced : pd.DataFrame
        Reduced feature matrix with only selected features.
    selection_freq : pd.Series
        Selection frequency per feature (index = feature name).
    selected_features : list[str]
        List of selected feature names.
    """
    rng = np.random.default_rng(random_state)
    real_features = X.columns.tolist()

    # Count in how many runs each feature beats the noise threshold
    beat_noise_counts = pd.Series(0, index=real_features, dtype=float)

    # Default XGBoost parameters for NAFS (can be overridden via xgb_kwargs)
    base_params: Dict[str, Any] = dict(
        objective="binary:logistic",
        eval_metric="logloss",
        n_jobs=-1,
        max_depth=3,
        learning_rate=0.05,
        n_estimators=300,
        subsample=0.8,
        colsample_bytree=0.8,
    )
    if xgb_kwargs:
        base_params.update(xgb_kwargs)

    for run in range(n_runs):
        X_shadow = X.copy()
        shadow_cols: List[str] = []

        # 1) Generate shadow features
        for m in range(noise_multiplier):
            for col in real_features:
                shadow_col = f"shadow_{m}_{col}"
                X_shadow[shadow_col] = rng.permutation(X[col].values)
                shadow_cols.append(shadow_col)

        # 2) Fit model on real + shadow features
        model = XGBClassifier(
            random_state=random_state + run,
            **base_params,
        )
        model.fit(X_shadow, y)

        # 3) Gain-based importances
        gain_dict = model.get_booster().get_score(importance_type="gain")
        gain_series = pd.Series(gain_dict, dtype=float)

        real_gain = gain_series.reindex(real_features).fillna(0.0)
        shadow_gain = gain_series.reindex(shadow_cols).fillna(0.0)

        # 4) Noise threshold from shadow gains
        if len(shadow_gain) > 0:
            shadow_threshold = shadow_gain.quantile(noise_quantile)
        else:
            # Safety fallback (should not happen in normal use)
            shadow_threshold = 0.0

        # 5) Increment count for features that beat the noise threshold
        beat_noise_counts += (real_gain > shadow_threshold).astype(int)

    selection_freq = beat_noise_counts / float(n_runs)

    selected_features = selection_freq.loc[
        selection_freq >= min_selection_freq
    ].index.tolist()

    # Fallback: if NAFS is too strict, keep all features
    if len(selected_features) == 0:
        selected_features = real_features

    X_reduced = X[selected_features].copy()

    return X_reduced, selection_freq, selected_features


# ============================================================
# 4) Importance-ordered correlation pruning (pandas + XGBoost)
# ============================================================

def importance_ordered_correlation_pruning(
    X: pd.DataFrame,
    y: pd.Series,
    xgb_params: Dict[str, Any],
    corr_threshold: float = 0.60,
    random_state: int = 42,
    scale_pos_weight: Optional[float] = None,
) -> Tuple[
    XGBClassifier,
    pd.DataFrame,
    pd.DataFrame,
    pd.DataFrame,
    List[str],
]:
    """
    Fit an XGBoost model, compute gain-based feature importances,
    build a correlation structure, and perform importance-ordered
    correlation pruning (greedy filter).

    Steps
    -----
    1) Fit XGBClassifier on X, y with provided `xgb_params` plus
       standard binary classification defaults (objective, eval_metric, etc.).
    2) Extract feature importances by gain and sort descending.
    3) Compute Pearson correlation matrix using only features with gain > 0.
    4) Flatten the upper triangle of the correlation matrix into a correlation
       vector (feature_i, feature_j, corr, abs_corr).
    5) Greedy feature selection:
       - Traverse features in descending order of gain.
       - Always keep the first (most important).
       - For each subsequent feature, keep it only if its maximum absolute
         correlation with the already selected set is below `corr_threshold`.

    This implements an "importance-ordered correlation pruning" that removes
    redundant features while respecting the initial ranking given by XGBoost.

    Parameters
    ----------
    X : pd.DataFrame
        Design matrix (features only).
    y : pd.Series
        Binary target (0/1).
    xgb_params : dict
        Hyperparameters for XGBClassifier (e.g., n_estimators, max_depth, etc.).
        Do not include "objective", "eval_metric", "n_jobs", "random_state",
        or "scale_pos_weight"; they are set internally by this function.
    corr_threshold : float
        Maximum allowed absolute correlation between any two selected features.
    random_state : int
        Random seed for XGBClassifier.
    scale_pos_weight : float, optional
        Class weight for the positive class. If None, it is computed as n_neg / n_pos.

    Returns
    -------
    model : XGBClassifier
        XGBClassifier fitted on the full X (before pruning).
    gain_df : pd.DataFrame
        DataFrame with columns ["feature", "gain"], sorted by gain descending.
    corr_matrix : pd.DataFrame
        Pearson correlation matrix for the gain-ordered features.
    corr_vector : pd.DataFrame
        Flattened upper triangle of the correlation matrix with columns:
          ["feature_i", "feature_j", "corr", "abs_corr"],
        sorted by abs_corr descending.
    selected_features : list[str]
        Features retained by the importance-ordered correlation pruning.
    """
    # ---------------------------------------------------------
    # 0) Compute scale_pos_weight if needed
    # ---------------------------------------------------------
    y = y.astype(int)
    pos = int(y.sum())
    neg = len(y) - pos

    if scale_pos_weight is None:
        scale_pos_weight = (neg / pos) if pos > 0 else 1.0

    # ---------------------------------------------------------
    # 1) Fit XGBClassifier with all features
    # ---------------------------------------------------------
    base_params = dict(
        objective="binary:logistic",
        eval_metric="logloss",
        random_state=random_state,
        n_jobs=-1,
        scale_pos_weight=scale_pos_weight,
    )
    base_params.update(xgb_params)

    model = XGBClassifier(**base_params)
    model.fit(X, y)

    # ---------------------------------------------------------
    # 2) Gain-based importances
    # ---------------------------------------------------------
    gain_dict = model.get_booster().get_score(importance_type="gain")
    gain_df = (
        pd.DataFrame(list(gain_dict.items()), columns=["feature", "gain"])
        .sort_values("gain", ascending=False)
        .reset_index(drop=True)
    )
    gain_features = [f for f in gain_df["feature"].tolist() if f in X.columns]

    # If no feature has gain, return empty structures
    if len(gain_features) == 0:
        corr_matrix = pd.DataFrame()
        corr_vector = pd.DataFrame(columns=["feature_i", "feature_j", "corr", "abs_corr"])
        selected_features: List[str] = []
        return model, gain_df, corr_matrix, corr_vector, selected_features

    # ---------------------------------------------------------
    # 3) Correlation matrix and vector
    # ---------------------------------------------------------
    X_gain = X[gain_features].copy()
    corr_matrix = X_gain.corr(method="pearson")

    # Upper triangle → vector of correlated pairs
    upper_mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    corr_vector = (
        corr_matrix.where(upper_mask)
        .stack()
        .reset_index()
        .rename(columns={"level_0": "feature_i", "level_1": "feature_j", 0: "corr"})
    )
    corr_vector["abs_corr"] = corr_vector["corr"].abs()
    corr_vector = corr_vector.sort_values("abs_corr", ascending=False).reset_index(drop=True)

    # ---------------------------------------------------------
    # 4) Greedy importance-ordered correlation pruning
    # ---------------------------------------------------------
    selected_features: List[str] = []

    for f in gain_features:
        if not selected_features:
            selected_features.append(f)
            continue

        # Maximum absolute correlation with already selected features
        max_abs_corr = corr_matrix.loc[f, selected_features].abs().fillna(0.0).max()
        if max_abs_corr < corr_threshold:
            selected_features.append(f)

    return model, gain_df, corr_matrix, corr_vector, selected_features


# ============================================================
# 5) Example usage (commented)
# ============================================================
#
# # Assume `pdf` is a pandas DataFrame with a binary target "default_flag"
# # and an ID column "id_cliente".
#
# # 1) Descriptive statistics
# desc_uni = describe_numeric_pandas(
#     df=pdf,
#     exclude=["id_cliente", "default_flag"]
# )
#
# # 2) Univariate Gini
# gini_uni = univariate_gini_pandas(
#     df=pdf,
#     target="default_flag",
#     exclude=["id_cliente"]
# )
#
# # 3) Combine univariate stats + Gini
# univariate = (
#     desc_uni
#     .merge(gini_uni, on="variable", how="left")
#     .sort_values("gini_no_missing", ascending=False)
# )
#
# # 4) NAFS-style feature reduction
# X = pdf.drop(columns=["default_flag"])
# y = pdf["default_flag"]
#
# X_nafs, sel_freq, feats_nafs = nafs_feature_reduction(
#     X=X,
#     y=y,
#     n_runs=20,
#     noise_multiplier=1,
#     noise_quantile=0.95,
#     min_selection_freq=0.60,
#     random_state=42,
# )
#
# # 5) Importance-ordered correlation pruning on NAFS-reduced features
# best_xgb_params = dict(
#     n_estimators=400,
#     learning_rate=0.05,
#     max_depth=3,
#     subsample=0.8,
#     colsample_bytree=0.8,
#     min_child_weight=3,
#     gamma=0.0,
#     reg_alpha=0.0,
#     reg_lambda=2.0,
#     grow_policy="depthwise",
#     max_leaves=0,
# )
#
# model_best, gain_df, corr_matrix, corr_vector, selected_feats = (
#     importance_ordered_correlation_pruning(
#         X=X_nafs,
#         y=y,
#         xgb_params=best_xgb_params,
#         corr_threshold=0.60,
#         random_state=42,
#     )
# )
#
# # 6) Final feature set for modeling
# X_final = X_nafs[selected_feats].copy()
