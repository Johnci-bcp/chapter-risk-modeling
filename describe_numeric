from pyspark.sql import functions as F
from pyspark.sql.types import NumericType

def describe_numeric_faster(df, exclude=None, rel_error=0.001, total_rows=None):
    """
    Faster numeric profiling using:
      - DataFrame.summary() for count, mean, stddev, min, max
      - approxQuantile() for p1, median, p99
      - Plus missing percentage per variable.

    Output columns (in this order):
      variable, n_non_missing, missing_pct,
      mean, stddev, min, max, p1, median, p99
    """

    exclude = set(exclude or [])
    numeric_cols = [
        f.name for f in df.schema.fields
        if isinstance(f.dataType, NumericType) and f.name not in exclude
    ]

    spark = df.sparkSession

    schema_str = (
        "variable string, "
        "n_non_missing double, missing_pct double, "
        "mean double, stddev double, min double, max double, "
        "p1 double, median double, p99 double"
    )

    if not numeric_cols:
        return spark.createDataFrame([], schema_str)

    # total_rows: used to compute missing percentage
    if total_rows is None:
        total_rows = df.count()

    if total_rows == 0:
        return spark.createDataFrame([], schema_str)

    # 1) Basic statistics via summary() (including count)
    basic = (
        df.select(*numeric_cols)
          .summary("count", "mean", "stddev", "min", "max")
    )

    # Convert wide summary to long format
    basic_long = (
        basic
        .selectExpr(
            "summary as stat",
            "stack({n}, {exprs}) as (variable, value)".format(
                n=len(numeric_cols),
                exprs=", ".join([f"'{c}', `{c}`" for c in numeric_cols])
            )
        )
        .groupBy("variable")
        .pivot("stat", ["count", "mean", "stddev", "min", "max"])
        .agg(F.first("value"))
    )

    # Cast stats to numeric (summary() returns strings)
    for col in ["count", "mean", "stddev", "min", "max"]:
        basic_long = basic_long.withColumn(col, F.col(col).cast("double"))

    # Non-missing count and missing percentage (0â€“100)
    basic_long = (
        basic_long
        .withColumn("n_non_missing", F.col("count"))
        .withColumn(
            "missing_pct",
            (F.lit(float(total_rows)) - F.col("n_non_missing")) / F.lit(float(total_rows)) * 100.0
        )
        .drop("count")
    )

    # 2) Percentiles via approxQuantile()
    quantiles = df.approxQuantile(numeric_cols, [0.01, 0.5, 0.99], rel_error)

    rows = []
    for col_name, qvals in zip(numeric_cols, quantiles):
        p1, p50, p99 = qvals
        rows.append((col_name, float(p1), float(p50), float(p99)))

    q_df = spark.createDataFrame(rows, ["variable", "p1", "median", "p99"])

    # 3) Combine both outputs and fix column order
    result = (
        basic_long
        .join(q_df, on="variable", how="inner")
        .select(
            "variable",
            "n_non_missing",
            "missing_pct",
            "mean",
            "stddev",
            "min",
            "max",
            "p1",
            "median",
            "p99",
        )
    )

    return result


from pyspark.sql import DataFrame
from pyspark.sql.types import NumericType
from functools import reduce

def describe_numeric_in_chunks(df, exclude=None, rel_error=0.001, chunk_size=50):
    """
    Chunk-based numeric profiling for very wide tables.

    Output columns (in this order):
      variable, n_non_missing, missing_pct,
      mean, stddev, min, max, p1, median, p99
    """

    exclude = set(exclude or [])
    numeric_cols = [
        f.name for f in df.schema.fields
        if isinstance(f.dataType, NumericType) and f.name not in exclude
    ]

    spark = df.sparkSession

    schema_str = (
        "variable string, "
        "n_non_missing double, missing_pct double, "
        "mean double, stddev double, min double, max double, "
        "p1 double, median double, p99 double"
    )

    if not numeric_cols:
        return spark.createDataFrame([], schema_str)

    total_rows = df.count()
    if total_rows == 0:
        return spark.createDataFrame([], schema_str)

    # Split numeric columns into chunks
    chunks = [
        numeric_cols[i:i + chunk_size]
        for i in range(0, len(numeric_cols), chunk_size)
    ]

    results = []

    for cols in chunks:
        sub_df = df.select(*cols)
        # NOTE: now describe_numeric_faster DOES accept total_rows
        res_chunk = describe_numeric_faster(
            sub_df,
            exclude=None,
            rel_error=rel_error,
            total_rows=total_rows
        )
        results.append(res_chunk)

    if not results:
        return spark.createDataFrame([], schema_str)

    combined = reduce(DataFrame.unionByName, results)

    combined = combined.select(
        "variable",
        "n_non_missing",
        "missing_pct",
        "mean",
        "stddev",
        "min",
        "max",
        "p1",
        "median",
        "p99",
    )

    return combined
