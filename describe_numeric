from pyspark.sql import DataFrame
from pyspark.sql.types import NumericType
from functools import reduce

def describe_numeric_in_chunks(df, exclude=None, rel_error=0.001, chunk_size=50):
    """
    Chunk-based numeric profiling for very wide tables.

    Output:
      variable, n_non_missing, missing_pct (0â€“100),
      mean, stddev, min, max, p1, median, p99
    """

    exclude = set(exclude or [])
    numeric_cols = [
        f.name for f in df.schema.fields
        if isinstance(f.dataType, NumericType) and f.name not in exclude
    ]

    spark = df.sparkSession

    schema_str = (
        "variable string, "
        "n_non_missing double, missing_pct double, "
        "mean double, stddev double, min double, max double, "
        "p1 double, median double, p99 double"
    )

    if not numeric_cols:
        return spark.createDataFrame([], schema_str)

    total_rows = df.count()
    if total_rows == 0:
        return spark.createDataFrame([], schema_str)

    # Split numeric columns into chunks
    chunks = [
        numeric_cols[i:i + chunk_size]
        for i in range(0, len(numeric_cols), chunk_size)
    ]

    results = []

    for cols in chunks:
        sub_df = df.select(*cols)
        res_chunk = describe_numeric_faster(
            sub_df,
            exclude=None,
            rel_error=rel_error,
            total_rows=total_rows  # reuse same total_rows
        )
        results.append(res_chunk)

    if not results:
        return spark.createDataFrame([], schema_str)

    combined = reduce(DataFrame.unionByName, results)

    return combined
