from pyspark.sql import functions as F
from pyspark.sql.types import NumericType

def describe_numeric_faster(df, exclude=None, rel_error=0.001):
    """
    Faster numeric profiling using:
      - DataFrame.summary() for mean, stddev, min, max
      - approxQuantile() for p1, median, p99

    Parameters
    ----------
    df : pyspark.sql.DataFrame
        Input Spark DataFrame.
    exclude : list of str, optional
        Columns to exclude.
    rel_error : float, optional
        Relative error for approxQuantile().
        Lower values increase accuracy but require more computation.

    Returns
    -------
    pyspark.sql.DataFrame
        Long-format DataFrame with statistics:
        [variable, mean, stddev, min, max, p1, median, p99]
    """

    exclude = set(exclude or [])
    numeric_cols = [
        f.name for f in df.schema.fields
        if isinstance(f.dataType, NumericType) and f.name not in exclude
    ]

    if not numeric_cols:
        return df.sparkSession.createDataFrame([],
            "variable string, mean double, stddev double, min double, max double, p1 double, median double, p99 double"
        )

    # Step 1: Basic statistics via summary()
    basic = (
        df.select(*numeric_cols)
          .summary("mean", "stddev", "min", "max")
    )

    # Convert wide summary to long format
    basic_long = (
        basic
        .selectExpr("summary as stat", "stack({n}, {exprs}) as (variable, value)".format(
            n=len(numeric_cols),
            exprs=", ".join([f"'{c}', `{c}`" for c in numeric_cols])
        ))
        .groupBy("variable")
        .pivot("stat", ["mean", "stddev", "min", "max"])
        .agg(F.first("value"))
    )

    # Cast to numeric
    for col in ["mean", "stddev", "min", "max"]:
        basic_long = basic_long.withColumn(col, F.col(col).cast("double"))

    # Step 2: Percentiles via approxQuantile()
    quantiles = df.approxQuantile(numeric_cols, [0.01, 0.5, 0.99], rel_error)

    rows = []
    for col_name, qvals in zip(numeric_cols, quantiles):
        p1, p50, p99 = qvals
        rows.append((col_name, float(p1), float(p50), float(p99)))

    spark = df.sparkSession
    q_df = spark.createDataFrame(rows, ["variable", "p1", "median", "p99"])

    # Step 3: Combine both outputs
    result = (
        basic_long
        .join(q_df, on="variable", how="inner")
    )

    return result
