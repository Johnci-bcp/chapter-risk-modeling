def describe_numeric_fast(df, exclude=None, accuracy=10000):
    """
    Estadísticos por columna numérica en una sola pasada (rápido).
    Devuelve: [variable, mean, stddev, min, max, p1, p50, p99]
    - exclude: lista de columnas a excluir
    - accuracy: buffer de percentile_approx (↑ = más preciso, más memoria)
    """
    from pyspark.sql import functions as F
    from pyspark.sql.types import NumericType

    exclude = set(exclude or [])
    numeric_cols = [f.name for f in df.schema.fields
                    if isinstance(f.dataType, NumericType) and f.name not in exclude]
    if not numeric_cols:
        return df.sparkSession.createDataFrame([], "variable string, mean double, stddev double, min double, max double, p1 double, p50 double, p99 double")

    # 1) Agregamos TODO en un único agg (un job)
    exprs = []
    for c in numeric_cols:
        exprs.extend([
            F.mean(c).alias(f"{c}__mean"),
            F.stddev_samp(c).alias(f"{c}__stddev"),
            F.min(c).alias(f"{c}__min"),
            F.max(c).alias(f"{c}__max"),
            F.expr(f"percentile_approx(`{c}`, 0.01, {accuracy})").alias(f"{c}__p1"),
            F.expr(f"percentile_approx(`{c}`, 0.50, {accuracy})").alias(f"{c}__p50"),
            F.expr(f"percentile_approx(`{c}`, 0.99, {accuracy})").alias(f"{c}__p99"),
        ])
    agg1 = df.agg(*exprs)

    # 2) Proyectamos el único registro en arrays alineados y explotamos (sin más scans)
    lit_vars = F.array(*[F.lit(c) for c in numeric_cols])
    arr_mean = F.array(*[F.col(f"{c}__mean")  for c in numeric_cols])
    arr_std  = F.array(*[F.col(f"{c}__stddev") for c in numeric_cols])
    arr_min  = F.array(*[F.col(f"{c}__min")   for c in numeric_cols])
    arr_max  = F.array(*[F.col(f"{c}__max")   for c in numeric_cols])
    arr_p1   = F.array(*[F.col(f"{c}__p1")    for c in numeric_cols])
    arr_p50  = F.array(*[F.col(f"{c}__p50")   for c in numeric_cols])
    arr_p99  = F.array(*[F.col(f"{c}__p99")   for c in numeric_cols])

    zipped = F.arrays_zip(lit_vars.alias("variable"),
                          arr_mean.alias("mean"),
                          arr_std.alias("stddev"),
                          arr_min.alias("min"),
                          arr_max.alias("max"),
                          arr_p1.alias("p1"),
                          arr_p50.alias("p50"),
                          arr_p99.alias("p99"))

    long_df = (agg1
               .select(F.explode(zipped).alias("z"))
               .select(
                   F.col("z.variable").alias("variable"),
                   F.col("z.mean").alias("mean"),
                   F.col("z.stddev").alias("stddev"),
                   F.col("z.min").alias("min"),
                   F.col("z.max").alias("max"),
                   F.col("z.p1").alias("p1"),
                   F.col("z.p50").alias("median"),
                   F.col("z.p99").alias("p99"),
               ))

    return long_df.orderBy(F.desc_nulls_last("median"))
