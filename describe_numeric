from pyspark.sql import functions as F
from pyspark.sql.types import NumericType
from pyspark.sql import SparkSession

def describe_numeric_faster_safe(df, exclude=None, rel_error=0.001):
    """
    Numeric profiling using only public PySpark APIs and avoiding df._jdf / df.sparkSession
      - df.summary() for count, mean, stddev, min, max
      - df.approxQuantile() for p1, median, p99
      - missing_pct computed from count / total_rows

    Output:
      variable, n_non_missing, missing_pct,
      mean, stddev, min, max, p1, median, p99
    """

    exclude = set(exclude or [])
    numeric_cols = [
        f.name for f in df.schema.fields
        if isinstance(f.dataType, NumericType) and f.name not in exclude
    ]

    # Usar la sesión activa en lugar de df.sparkSession
    spark = SparkSession.getActiveSession()
    if spark is None:
        raise RuntimeError("No active SparkSession found. Make sure you are in a Spark environment.")

    schema_str = (
        "variable string, "
        "n_non_missing double, missing_pct double, "
        "mean double, stddev double, min double, max double, "
        "p1 double, median double, p99 double"
    )

    if not numeric_cols:
        return spark.createDataFrame([], schema_str)

    total_rows = df.count()
    if total_rows == 0:
        return spark.createDataFrame([], schema_str)

    # 1) summary para count, mean, stddev, min, max
    basic = (
        df.select(*numeric_cols)
          .summary("count", "mean", "stddev", "min", "max")
    )

    basic_long = (
        basic
        .selectExpr(
            "summary as stat",
            "stack({n}, {exprs}) as (variable, value)".format(
                n=len(numeric_cols),
                exprs=", ".join([f"'{c}', `{c}`" for c in numeric_cols])
            )
        )
        .groupBy("variable")
        .pivot("stat", ["count", "mean", "stddev", "min", "max"])
        .agg(F.first("value"))
    )

    for col in ["count", "mean", "stddev", "min", "max"]:
        basic_long = basic_long.withColumn(col, F.col(col).cast("double"))

    basic_long = (
        basic_long
        .withColumn("n_non_missing", F.col("count"))
        .withColumn(
            "missing_pct",
            (F.lit(float(total_rows)) - F.col("n_non_missing")) / F.lit(float(total_rows)) * 100.0
        )
        .drop("count")
    )

    # 2) approxQuantile para p1, median, p99
    quantiles = df.approxQuantile(numeric_cols, [0.01, 0.5, 0.99], rel_error)

    rows = []
    for col_name, qvals in zip(numeric_cols, quantiles):
        p1, p50, p99 = qvals
        rows.append((col_name, float(p1), float(p50), float(p99)))

    q_df = spark.createDataFrame(rows, ["variable", "p1", "median", "p99"])

    result = (
        basic_long
        .join(q_df, on="variable", how="inner")
        .select(
            "variable",
            "n_non_missing",
            "missing_pct",
            "mean",
            "stddev",
            "min",
            "max",
            "p1",
            "median",
            "p99"
        )
    )

    return result




from pyspark.sql import DataFrame
from pyspark.sql.types import NumericType
from pyspark.sql import SparkSession
from functools import reduce

def describe_numeric_in_chunks_safe(df, exclude=None, rel_error=0.001, chunk_size=50):
    """
    Chunk-based numeric profiling for very wide tables, using describe_numeric_faster_safe.
    Avoids df.sparkSession / df._jdf (compatible with shared clusters + Unity Catalog).
    """

    exclude = set(exclude or [])
    numeric_cols = [
        f.name for f in df.schema.fields
        if isinstance(f.dataType, NumericType) and f.name not in exclude
    ]

    spark = SparkSession.getActiveSession()
    if spark is None:
        raise RuntimeError("No active SparkSession found. Make sure you are in a Spark environment.")

    schema_str = (
        "variable string, "
        "n_non_missing double, missing_pct double, "
        "mean double, stddev double, min double, max double, "
        "p1 double, median double, p99 double"
    )

    if not numeric_cols:
        return spark.createDataFrame([], schema_str)

    # Particionamos solo por columnas
    chunks = [
        numeric_cols[i:i + chunk_size]
        for i in range(0, len(numeric_cols), chunk_size)
    ]

    results = []

    for cols in chunks:
        sub_df = df.select(*cols)
        res_chunk = describe_numeric_faster_safe(
            sub_df,
            exclude=None,
            rel_error=rel_error
        )
        results.append(res_chunk)

    if not results:
        return spark.createDataFrame([], schema_str)

    combined = reduce(DataFrame.unionByName, results)

    combined = combined.select(
        "variable",
        "n_non_missing",
        "missing_pct",
        "mean",
        "stddev",
        "min",
        "max",
        "p1",
        "median",
        "p99",
    )

    return combined




desc_uni = describe_numeric_in_chunks_safe(
    df=credit_df,
    exclude=["id_cliente", "default_flag"],
    rel_error=1e-3,
    chunk_size=50
)

desc_uni.show(20, truncate=False)



[JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute '_jdf' is not supported on shared clusters. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog
[JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute '_jdf' is not supported on shared clusters. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://learn.microsoft.com/azure/databricks/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog


from typing import List, Optional
from pyspark.sql import functions as F, Window
from pyspark.sql.types import NumericType

def gini_by_numeric_features_fast(
    df,
    label: str,
    positive=1,
    exclude: Optional[List[str]] = None,
    bins: int = 128,
    rel_err: float = 1e-3,
    unpivot_strategy: str = "stack",
    fillna_zero: bool = False,
    sample_frac: Optional[float] = None,
    sample_seed: int = 42,
):
    """
    Compute per-feature AUC and Gini (2*AUC-1) for many numeric columns at scale,
    using quantile-based pre-binning (approxQuantile) + Mann–Whitney formulation.
    No UDFs. Supports optional missing-value handling and row sampling.

    Parameters
    ----------
    df : pyspark.sql.DataFrame
        Input DataFrame.
    label : str
        Binary target column name.
    positive : Any
        Value to be treated as positive class (default=1).
    exclude : list[str], optional
        Columns to skip (IDs, leakage vars, etc.).
    bins : int
        Number of quantile bins per feature (typ. 32–128; lower is faster).
    rel_err : float
        Relative error for approxQuantile (smaller -> more precise, higher cost).
    unpivot_strategy : {"stack", "loop"}
        "stack": single wide→long unpivot + one big aggregation
        "loop" : per-feature light job (safer when you have a very wide table).
    fillna_zero : bool
        If True, feature nulls are imputed to 0.0 before binning.
        If False, rows with null feature values are excluded.
    sample_frac : float or None
        Optional fraction of rows to sample for speed (e.g., 0.2). If None, use full data.
    sample_seed : int
        Random seed for sampling.

    Returns
    -------
    pyspark.sql.DataFrame
        Columns: [variable, auc, gini, direction, n, n_pos, n_neg, bins_used]
    """

    # ---------------- Helpers ----------------
    def _build_bin_expr(col, splits, fillna_zero=False):
        """
        Build native CASE WHEN expression returning bin index for `col` given sorted `splits`.
        Bins cover [splits[i], splits[i+1]) except the last, which includes the right bound.

        If fillna_zero=True, nulls are first coalesced to 0.0.
        """
        if fillna_zero:
            col = F.coalesce(col, F.lit(0.0))

        # Ensure -inf / +inf guard rails
        if splits[0] != float("-inf"):
            splits = [float("-inf")] + splits
        if splits[-1] != float("inf"):
            splits = splits + [float("inf")]

        expr = None
        last_idx = len(splits) - 2  # num_bins = len(splits)-1
        for i in range(len(splits) - 1):
            left = F.lit(splits[i])
            right = F.lit(splits[i + 1])
            cond = (col >= left) & (col < right) if i < last_idx else (col >= left) & (col <= right)
            expr = F.when(cond, F.lit(i)).otherwise(expr) if expr is not None else F.when(cond, F.lit(i))
        return expr  # null if no match

    def _auc_from_binned(agg_df, n, n_pos, n_neg):
        """
        Given (variable, bin, cnt, pos, bins_used), compute AUC/Gini with Mann–Whitney.
        """
        w = Window.partitionBy("variable").orderBy("bin")
        cum_cnt = F.sum("cnt").over(w)
        cum_before = cum_cnt - F.col("cnt")
        avg_rank = (cum_before + F.lit(1.0) + cum_before + F.col("cnt")) / 2.0
        sum_ranks_pos = F.sum(avg_rank * F.col("pos")).over(Window.partitionBy("variable"))

        auc_asc = (sum_ranks_pos - F.lit(n_pos * (n_pos + 1) / 2.0)) / F.lit(n_pos * n_neg)

        one = (agg_df
               .withColumn("auc_asc", auc_asc)
               .groupBy("variable")
               .agg(F.max("auc_asc").alias("auc_asc"),
                    F.max("bins_used").alias("bins_used")))

        one = one.withColumn("auc", F.when(F.col("auc_asc") >= 0.5, F.col("auc_asc"))
                                       .otherwise(1.0 - F.col("auc_asc")))
        one = one.withColumn("gini", 2.0 * F.col("auc") - 1.0)
        one = one.withColumn(
            "direction",
            F.when(F.col("auc_asc") >= 0.5, F.lit("ascending"))
             .otherwise(F.lit("descending"))
        )
        one = (one
               .withColumn("n", F.lit(n))
               .withColumn("n_pos", F.lit(n_pos))
               .withColumn("n_neg", F.lit(n_neg)))
        return one.select("variable", "auc", "gini", "direction", "n", "n_pos", "n_neg", "bins_used")

    # ---------------- 0) Prep & guards ----------------
    exclude = set(exclude or [])
    numeric_cols = [
        f.name for f in df.schema.fields
        if isinstance(f.dataType, NumericType) and f.name not in exclude and f.name != label
    ]

    if not numeric_cols:
        return df.sparkSession.createDataFrame(
            [], "variable string, auc double, gini double, direction string, "
                "n long, n_pos long, n_neg long, bins_used int"
        )

    y = (F.col(label) == F.lit(positive)).cast("int").alias("_y_")
    base = df.select(*numeric_cols, y)

    # Optional sampling to speed up everything
    if sample_frac is not None and 0.0 < sample_frac < 1.0:
        base = base.sample(withReplacement=False, fraction=sample_frac, seed=sample_seed)

    # n, n_pos, n_neg on the (possibly) sampled base
    n_row = base.agg(F.count("*").alias("n"), F.sum("_y_").alias("n_pos")).first()
    n = int(n_row["n"] or 0)
    n_pos = int(n_row["n_pos"] or 0)
    n_neg = n - n_pos
    if n == 0 or n_pos == 0 or n_neg == 0:
        return df.sparkSession.createDataFrame(
            [], "variable string, auc double, gini double, direction string, "
                "n long, n_pos long, n_neg long, bins_used int"
        )

    # ---------------- 1) Quantile splits (one call) ----------------
    qs = [i / float(bins) for i in range(bins + 1)]
    edges_all = base.approxQuantile(numeric_cols, qs, rel_err)  # one job

    col_splits = {}
    for c, edges in zip(numeric_cols, edges_all):
        uniq = sorted(set(edges))
        if len(uniq) >= 2:
            col_splits[c] = uniq

    usable_cols = list(col_splits.keys())
    if not usable_cols:
        return df.sparkSession.createDataFrame(
            [], "variable string, auc double, gini double, direction string, "
                "n long, n_pos long, n_neg long, bins_used int"
        )

    # ---------------- 2) Bin assignment ----------------
    binned = base
    for c in usable_cols:
        binned = binned.withColumn(
            f"{c}__bin",
            _build_bin_expr(F.col(c), col_splits[c], fillna_zero=fillna_zero)
        )

    # ---------------- 3A) Unpivot strategy: stack ----------------
    if unpivot_strategy == "stack":
        parts = []
        for c in usable_cols:
            parts += [f"'{c}'", f"`{c}__bin`"]
        stack_expr = f"stack({len(usable_cols)}, " + ", ".join(parts) + ") as (variable, bin)"

        long_df = binned.selectExpr("_y_", stack_expr).where(F.col("bin").isNotNull())

        agg = (long_df
               .groupBy("variable", "bin")
               .agg(F.count("*").alias("cnt"),
                    F.sum("_y_").alias("pos")))
        bins_used = agg.groupBy("variable").agg(F.count("*").alias("bins_used"))
        agg = agg.join(bins_used, "variable", "left")

        return _auc_from_binned(agg, n, n_pos, n_neg)

    # ---------------- 3B) Unpivot strategy: loop ----------------
    elif unpivot_strategy == "loop":
        out = None
        for c in usable_cols:
            bc = f"{c}__bin"
            agg = (binned
                   .select("_y_", F.col(bc).alias("bin"))
                   .where(F.col("bin").isNotNull())
                   .groupBy("bin")
                   .agg(F.count("*").alias("cnt"),
                        F.sum("_y_").alias("pos"))
                   .withColumn("variable", F.lit(c)))
            agg = agg.withColumn("bins_used", F.count("*").over(Window.partitionBy("variable")))
            res = _auc_from_binned(agg, n, n_pos, n_neg)
            out = res if out is None else out.unionByName(res, allowMissingColumns=True)

        return out.orderBy(F.desc("gini")) if out is not None else df.sparkSession.createDataFrame(
            [], "variable string, auc double, gini double, direction string, "
                "n long, n_pos long, n_neg long, bins_used int"
        )

    else:
        raise ValueError("unpivot_strategy must be 'stack' or 'loop'")


def gini_by_numeric_features_with_missing(
    df,
    label: str,
    positive=1,
    exclude: Optional[List[str]] = None,
    bins: int = 128,
    rel_err: float = 1e-3,
    unpivot_strategy: str = "stack",
    sample_frac: Optional[float] = None,
    sample_seed: int = 42,
):
    """
    Compute two Ginis per numeric feature:
      - gini_no_missing   : dropping rows where feature is null
      - gini_missing_zero : imputing null feature values with 0.0

    Fully supports `exclude`, which is applied in both calculations.
    """

    # normalizamos exclude
    exclude = set(exclude or [])

    # ============================================================
    # 1) Gini excluyendo missing
    # ============================================================
    res_no = gini_by_numeric_features_fast(
        df=df,
        label=label,
        positive=positive,
        exclude=exclude,            # <-- NOW PASSED CORRECTLY
        bins=bins,
        rel_err=rel_err,
        unpivot_strategy=unpivot_strategy,
        fillna_zero=False,
        sample_frac=sample_frac,
        sample_seed=sample_seed
    ).withColumnRenamed("auc", "auc_no_missing") \
     .withColumnRenamed("gini", "gini_no_missing") \
     .withColumnRenamed("direction", "direction_no_missing") \
     .withColumnRenamed("bins_used", "bins_used_no_missing")

    # ============================================================
    # 2) Gini imputando missing como 0
    # ============================================================
    res_zero = gini_by_numeric_features_fast(
        df=df,
        label=label,
        positive=positive,
        exclude=exclude,           # <-- NOW PASSED CORRECTLY
        bins=bins,
        rel_err=rel_err,
        unpivot_strategy=unpivot_strategy,
        fillna_zero=True,
        sample_frac=sample_frac,
        sample_seed=sample_seed
    ).select(
        "variable", "auc", "gini", "direction", "bins_used"
    ).withColumnRenamed("auc", "auc_missing_zero") \
     .withColumnRenamed("gini", "gini_missing_zero") \
     .withColumnRenamed("direction", "direction_missing_zero") \
     .withColumnRenamed("bins_used", "bins_used_missing_zero")

    # ============================================================
    # 3) Join results
    # ============================================================
    out = (
        res_no.join(res_zero, on="variable", how="left")
              .select(
                  "variable",
                  "auc_no_missing", "gini_no_missing", "direction_no_missing",
                  "auc_missing_zero", "gini_missing_zero", "direction_missing_zero",
                  "n", "n_pos", "n_neg",
                  "bins_used_no_missing", "bins_used_missing_zero"
              )
    )

    return out

from pyspark.sql import functions as F

gini_uni = gini_by_numeric_features_with_missing(
    df=df_credit,
    label="default_flag",
    exclude=["id_cliente", "fecha", "cod_producto", "default_flag"],
    bins=64,
    rel_err=1e-3,
    unpivot_strategy="loop",
    sample_frac=0.3     # si la base es pesada
)

gini_uni.orderBy(F.desc("gini_no_missing")).show(40, truncate=False)



