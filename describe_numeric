def describe_numeric(df, exclude=None, quantiles=(0.01, 0.5, 0.99), rel_err=0.0):
    """
    Devuelve un DataFrame Spark con [variable, mean, stddev, min, max, p<q>...]
    - exclude: lista de columnas a excluir
    - quantiles: tupla/lista de cuantiles (e.g., (0.01, 0.5, 0.99))
    - rel_err: error relativo para approxQuantile (0.0 = exacto)
    """
    from pyspark.sql import functions as F, Row
    from pyspark.sql.types import NumericType

    exclude = set(exclude or [])
    numeric_cols = [f.name for f in df.schema.fields
                    if isinstance(f.dataType, NumericType) and f.name not in exclude]

    if not numeric_cols:
        return df.sparkSession.createDataFrame([], schema="variable string")

    # 1) Estadísticos básicos en una sola agregación (un job)
    exprs = []
    for c in numeric_cols:
        exprs.extend([
            F.mean(c).alias(f"{c}_mean"),
            F.stddev(c).alias(f"{c}_stddev"),
            F.min(c).alias(f"{c}_min"),
            F.max(c).alias(f"{c}_max"),
        ])
    basic_row = df.agg(*exprs).collect()[0]  # una sola pasada y un solo collect

    # 2) Quantiles para todas las columnas en un solo llamado (otro job)
    q_list = list(quantiles)
    q_values_all = df.approxQuantile(numeric_cols, q_list, rel_err)

    # 3) Construcción del resultado fila a fila (sin más jobs)
    rows = []
    for i, c in enumerate(numeric_cols):
        q_vals = q_values_all[i]  # [q1, q2, q3, ...] para la columna c
        row_dict = {
            "variable": c,
            "mean": basic_row[f"{c}_mean"],
            "stddev": basic_row[f"{c}_stddev"],
            "min": basic_row[f"{c}_min"],
            "max": basic_row[f"{c}_max"],
        }
        # agrega p<q> dinámicamente (p1, p50, p99, etc.)
        for q, v in zip(q_list, q_vals):
            # nombres legibles: 0.01 -> p1, 0.5 -> p50, 0.99 -> p99
            pct = int(round(q * 100))
            row_dict[f"p{pct}"] = v
        rows.append(Row(**row_dict))

    return df.sparkSession.createDataFrame(rows)
